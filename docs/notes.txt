endianness_map = {
    '>': 'big',
    '<': 'little',
    '=': sys.byteorder,
    '|': 'not applicable',
}


https://martinheinz.dev/blog/32
class Person:
    def __init__(self, **kwargs):
        self.__dict__.update(**kwargs)

class Person:
    def __init__(self, **kwargs):
        vars(self).update(**kwargs)  # Alternatively use `vars()`
The snippet above demonstrates usage of self.__dict__ which is a dictionary which stores all the attributes of class (unless __slots__ is declared)



https://stackoverflow.com/questions/5268404/what-is-the-fastest-way-to-check-if-a-class-has-a-function-defined
invert_op = getattr(self, "invert_op", None)
if callable(invert_op):
    invert_op(self.path.parent_op)



https://stackoverflow.com/questions/5910703/how-to-get-all-methods-of-a-python-class-with-given-decorator




#sys.path.append(os.path.abspath('src'))
#sys.path.append(os.path.abspath('adt'))
#sys.path.append(os.path.abspath('core'))
#sys.path.append(os.path.abspath('patterns'))
#sys.path.append(os.path.abspath('algorithms'))


import importlib
 modnames = ["os", "sys", "math"]
 for lib in modnames:
     globals()[lib] = importlib.import_module(lib)



# How to merge two dictionaries
# in Python 3.5+
>>> x = {'a': 1, 'b': 2}
>>> y = {'b': 3, 'c': 4}
>>> z = {**x, **y}
>>> z
{'c': 4, 'a': 1, 'b': 3}



# Different ways to test multiple
# flags at once in Python
x, y, z = 0, 1, 0

if x == 1 or y == 1 or z == 1:
    print('passed')

if 1 in (x, y, z):
    print('passed')

# These only test for truthiness:
if x or y or z:
    print('passed')

if any((x, y, z)):
    print('passed')





    @staticmethod
    def collect_predecessors(vertex, graph):
        """
        """
        assert vertex is not None, "Invalid argument 'vertex'"
        assert vertex.id in graph.vertices, f"Missing vertex: {vertex}"
        result = list()
        if graph.digraph:
            pass
        else:
            edges = graph.get_edges(vertex)
            for edge in edges:
                vertex_u = edge.endpoints[0]
                vertex_v = edge.endpoints[1]
                if vertex == vertex_v:  # incoming edge
                    result.append(vertex_u)
        return result




# How to sort a Python dict by value
# (== get a representation sorted by value)

>>> xs = {'a': 4, 'b': 3, 'c': 2, 'd': 1}

>>> sorted(xs.items(), key=lambda x: x[1])
[('d', 1), ('c', 2), ('b', 3), ('a', 4)]

# Or:

>>> import operator
>>> sorted(xs.items(), key=operator.itemgetter(1))
[('d', 1), ('c', 2), ('b', 3), ('a', 4)]




        collected_vertices = list()
        dfs_visitor = DfsVisitor(graph)
        for vertex in graph.vertices.values():
            if (vertex.flags & Flags.VISITED) != Flags.VISITED:
                v = dfs_visitor.visit(vertex, callback=lambda v: collected_vertices.append(v))
                collected_vertices.append(next(v))
        assert collected_vertices == [v1, v2, v3]





# The get() method on dicts
# and its "default" argument
name_for_userid = {
    382: "Alice",
    590: "Bob",
    951: "Dilbert",
}

def greeting(userid):
    return "Hi %s!" % name_for_userid.get(userid, "there")

>>> greeting(382)
"Hi Alice!"
>>> greeting(333333)
"Hi there!"





            for vertex in graph.vertices.values():
                if (vertex.flags & Flags.VISITED) != Flags.VISITED:
                    for v in GraphAlgorithms.dfs(vertex):
                        collected_vertices.append(v)
            assert len(collected_vertices) == len(vertices)




reverse list
[result[k] for k in range(len(result)-1, -1, -1)]




        for i in range(len(self._vertices)):
            result.append([0 for i in range(len(self._vertices))])
        for edge in self._edges.values():
            result[edge.endpoints[0].id][edge.endpoints[1].id] = edge.value if edge.value else 1



https://realpython.com/primer-on-python-decorators/
import functools

def debug(func):
    """Print the function signature and return value"""
    @functools.wraps(func)
    def wrapper_debug(*args, **kwargs):
        args_repr = [repr(a) for a in args]                      # 1
        kwargs_repr = [f"{k}={v!r}" for k, v in kwargs.items()]  # 2
        signature = ", ".join(args_repr + kwargs_repr)           # 3
        print(f"Calling {func.__name__}({signature})")
        value = func(*args, **kwargs)
        print(f"{func.__name__!r} returned {value!r}")           # 4
        return value
    return wrapper_debug

import math
from decorators import debug

# Apply a decorator to a standard library function
math.factorial = debug(math.factorial)

def approximate_e(terms=18):
    return sum(1 / math.factorial(n) for n in range(terms))






        def dfs(node):
            stack.append(node)
            nodes.append(node)
            for kid in node.kids:
                dfs(kid)
                nodes.append(node)
            stack.pop()

        dfs(tree)







    @property
    def leaf(self):
        return self.degree == 1 or self.degree == 0




2D array:
a = [[0 for column in range(columns)] for row in range(rows)]
a = [[0] * cols for _ in range(rows)]



import heapq
pqueue = list()  # priority queue
heapq.heappush(pqueue, (10, 'v2'))
heapq.heappush(pqueue, (10, 'v1'))
heapq.heappush(pqueue, (0, 'v3'))
value, vertex = heapq.heappop(pqueue)
value, vertex




https://github.com/abarnert/treestuff/blob/master/treestuff.py
def postorder(node, children_func):
    for child in children_func(node):
        yield from postorder(child, children_func)
    yield node

# ... or iteratively:

def preorder(node, children_func):
    s = [node]
    while s:
        node = s.pop()
        yield node
        s.extend(reversed(list(children_func(node))))

# ... and of course it works just as well for BFS as fro DFS:

def levelorder(node, children_func):
    q = collections.deque([node])
    while q:
        node = q.popleft()
        yield node
        q.extend(children_func(node))





        ss = SuffixArray.concatenate_strings(['abca', 'bcad', 'daca'])



        # full_size = core_size + sum(map(len, keys))


мерджить словари
cities_us = {'New York City': 'US', 'Los Angeles': 'US'}
cities_uk = {'London': 'UK', 'Birmingham': 'UK'}
cities = cities_us|cities_uk
print(cities)


A = [1, 2, 3]
B = (4, 5, 6)
C = {7, 8, 9}
L = [*A, *B, *C]
print(L)





import itertools
from typing import Iterable

def pairwise(iterable: Iterable):
    """Iterate over elements two by two.

    s -> (s0,s1), (s1,s2), (s2, s3), ...
    """
    a, b = itertools.tee(iterable)
    next(b, None)
    return zip(a, b)

def iter_slice(iterable: bytes, n: int):
    """Yield slices of size n and says if each slice is the last one.

    s -> (b'123', False), (b'45', True)
    """
    start = 0
    stop = start + n
    final_offset = len(iterable)

    while True:
        if start >= final_offset:
            break

        rv = iterable[start:stop]
        start = stop
        stop = start + n
        yield rv, start >= final_offset




    def split_entries(self) -> list:
        """Split the entries in half.

        Keep the lower part in the node and return the upper one.
        """
        len_entries = len(self.entries)
        rv = self.entries[len_entries//2:]
        self.entries = self.entries[:len_entries//2]
        assert len(self.entries) + len(rv) == len_entries
        return rv







def literal_patterns(subject):
    match subject:
        case 1:
            print("The subject matches with 1")
        case 1.0:
            print("The subject matches with 1.0")
        case 2+3j:
            print(f"The real and imaginary parts are 2 and 3 respectively.")
        case "Masashi Kishimoto":
            print(f"Masashi Kishimoto is the author of manga series which was first published in 1999.")
        case None:
            print("The subject matches with None")
        case True:
            print("The subject matches with True")


def capture_patterns(subject):

    match subject:
        case [a,]:
            print(f"Collection has only one entry: {a}")
        case (a, b):
            print(f"Find flowrate for volume {a}litres and time {b}secs")
        case value:
            print(f"{value} is the value of subject")



def wildcard_patterns(subject):

    match subject:
        case (1,_,1):
            print(f"{subject} is a palindrome collection")
        case (_,_):
            print(f"{subject} is a collection of 2")
        case _:
            print("Unknown Match")



def as_patterns(subject):

    match subject:
        case "Kg" as mass:
            print(f"SI unit for mass is {mass}")
        case "N" as a force:
            print(f"SI unit for force is {force}")


def or_patterns(subject):

    match subject:
        case "RED" | "YELLOW" | "BLUE" as p_colour:
            print(f"{p_colour} is one of the primary colours")
        case colour:
            print(f"{colour} is not one of the primary colours")



def guard(subject):

    match subject:
        case int(number) if number % 2:
            print(f"{number} is an odd integer")
        case int(number):
            print(f"{number} is an even integer")
        case element:
            print(f"{element} is not an integer")






    # def revoke_codepoint(self):
    #     """
    #     """
    #     if self._content_position > self._start_content:
    #         self._content_position -= 1
    #     result = self._content[self._content_position]
    #     return result




        self._unicode = False  # true if the last obtained codepoint from unicode-escape
        self._unicode_length = 0  # length of unicode codepoint






    def advance(self):
        """
        Content is represented as string of codepoints.
        """
        self._content_position += 1
        if self._content_position < self._end_content:
            self._codepoint = self._content.data[self._content_position]
            if Text.back_slash(self._codepoint):
                if self._unicode_backslash_count % 2 == 0:  # '\' might start unicode escape sequence
                    prefix = self.peek()  # check for single '\': ..._count = 0, 2, etc.
                    if Tokenizer.unicode_escape_prefix(prefix):
                        mode = 'u' if prefix == 0x00000075 else 'U'
                        self._codepoint, self._content_position =\
                            self.consume_unicode_escape(mode, self._content_position)
                    else:
                        self._unicode_backslash_count += 1
                else:
                    self._unicode_backslash_count += 1  # single '\'
                    self._codepoint, self._content_position = self.consume_escape()
            else:
                self._unicode_backslash_count = 0
        else:
            self._codepoint = Text.eos_codepoint()
        if self._content_position > self._end_content:
            self._content_position = self._end_content
        assert Text.valid_codepoint(self._codepoint)
        return self._codepoint









    def scan_number(self):
        """
        Binary:      0b101111100011   0b__101_1_1_1100_011
        Octal:       0o5743 or 05743  0o_57__4_3 or 0__57_4____3
        Decimal:     3043             3___0__4_3
        Hexadecimal: 0xBE3            0xB__E_3
        Real:        3.14159265359  3.1415E2    3.1415e2    3_5.1__41_5E2    3.1_41_5e2
                     3.141__26_3_9  3.1415E+2   3.1415e+2   3_6.1__41_5E+2   3.1_41_5e+2
                     3.141_______5  3.1415E-2   3.1415e-2   3_7.1__41_5E-2   3.1_41_5e-2
        Digit separator: _
        Not allowed at the beginning, before fraction, inside fraction ot at the end.
        Illegals: _10, 10_, 10_.5, 10._5, 10.e_-5, 10.e+5_34
        All numbers are 64 bits.
        """
        value = list()
        octal_prefix = False  # true if 0xxx
        codepoint = self._codepoint
        if Text.ascii_zero_digit(codepoint):  # only consider ASCII numbers
            match self.advance():
                case (0x00000062 | 0x00000042):  # b or B
                    radix = 2
                    digits = Text.ascii_binary_digit
                    self.advance()
                case (0x0000006F | 0x0000004F):  # o or O
                    radix = 8
                    digits = Text.ascii_octal_digit
                    self.advance()
                case (0x00000078 | 0x00000058):  # x or X
                    radix = 16
                    digits = Text.ascii_hexadecimal_digit
                    self.advance()
                case _:
                    # no need to advance as 0 already consumed
                    radix = 8
                    digits = Text.ascii_octal_digit
                    octal_prefix = True
                    value.append(codepoint)
        else:
            radix = 10
            digits = Text.ascii_decimal_digit
        valid = True  # track erroneous or not status
        fraction = False  # everything after . fraction, switching to real
        exponent = False  # true if consumed exponent e|E
        exponent_sign = False  # true if consumed exponent sign, + or -
        if (not octal_prefix and
                (ArtTokenizer.digits_separator(self._codepoint) or
                 ArtTokenizer.fraction_start(self._codepoint) or
                 ArtTokenizer.exponent_start(self._codepoint) or
                 ArtTokenizer.exponent_sign(self._codepoint))):
            valid = False  # separator(s), fraction, exponent, exponent sign cannot start number
        while (valid and
               not Text.eos(self._codepoint) and
               (digits(self._codepoint) or
                ArtTokenizer.digits_separator(self._codepoint) or
                ArtTokenizer.fraction_start(self._codepoint) or
                ArtTokenizer.exponent_start(self._codepoint) or
                ArtTokenizer.exponent_sign(self._codepoint))):
            if ArtTokenizer.digits_separator(self._codepoint):
                # if fraction:
                #     valid = False  # no separators in fraction
                #     break
                # else:
                self.advance()
                continue
            if ArtTokenizer.fraction_start(self._codepoint):
                if ((radix != 10 and  # only decimals
                     (radix == 8 and not octal_prefix)) or  # or octal as decimal
                        fraction):  # already parsing fraction
                    valid = False
                    break
                fraction = True
            elif ArtTokenizer.exponent_start(self._codepoint):
                if ((radix != 10 and  # only decimals
                     (radix == 8 and not octal_prefix)) or  # or octal as decimal
                        exponent):  # already parsing exponent
                    valid = False
                    break
                exponent = True
            elif ArtTokenizer.exponent_sign(self._codepoint):
                if ((radix != 10 and  # only decimals
                     (radix == 8 and not octal_prefix)) or  # or octal as decimal
                        exponent_sign):  # already seen exponent sign
                    valid = False
                    break
                exponent_sign = True
            value.append(self._codepoint)
            self.advance()
        if valid and (ArtTokenizer.digits_separator(self._codepoint) or
                      ArtTokenizer.fraction_start(self._codepoint) or
                      ArtTokenizer.exponent_start(self._codepoint) or
                      ArtTokenizer.exponent_sign(self._codepoint)):
            valid = False  # # separator(s), fraction, exponent, exponent sign cannot end number
        if valid:
            try:
                value = ''.join(map(str, map(chr, value)))
                if fraction:
                    self._token.kind = TokenKind.REAL
                    self._token.value = float(value)
                else:
                    self._token.kind = TokenKind.INTEGER
                    self._token.value = int(value, radix)
            except ValueError as ex:
                valid = False
        if not valid:
            self._diagnostics.add(Status(f'Invalid numeric literal at '
                                         f'{self.content.get_location(self._content_position)}',
                                         'tokenizer',
                                         Status.INVALID_REAL_LITERAL if fraction else Status.INVALID_INT_LITERAL))



            assert (indent % self._indent_size == 0,
                    f"Invalid indent, must be multiple of {self._indent_size}.")










    def process_indentation(self):
        """
        """
        if self._dedents:
            self._token.kind = self._dedents.popleft()
        else:
            if self._beginning_of_line:
                content_position = self._content_position
                codepoint = self._codepoint
                self._beginning_of_line = False
                if self._content_position < self._end_content:
                    indent = 0
                    while (self._content_position < self._end_content and
                           self._codepoint == 0x00000020):  # ' ':
                        self.advance()
                        indent += 1
                    ignore = ((indent >= 0 and Text.eol(self._codepoint)) or  # blank line, either '\n' or '   \n'
                              self.comment_start())  # comment
                    if not ignore and self._nesting_level == 0:
                        if indent == self._indents[self._indents_level]:
                            pass
                        elif indent > self._indents[self._indents_level]:
                            self._indents_level += 1
                            self._indents.append(indent)
                            self._token.kind = TokenKind.INDENT
                        else:  # indent < self._indents[self._indents_level]
                            n = 0
                            while (self._indents_level > 0 and
                                   indent < self._indents[self._indents_level]):
                                n += 1
                                self._indents_level -= 1
                                self._indents.pop()
                            for _ in range(n - 1):
                                self._dedents.append(TokenKind.DEDENT)
                            if indent == self._indents[self._indents_level]:
                                self._token.kind = TokenKind.DEDENT
                            else:
                                self._token.kind = TokenKind.CORRUPTED_DEDENT
                    if not (self._token.kind == TokenKind.INDENT or
                            self._token.kind == TokenKind.DEDENT):
                        self._content_position = content_position  # rollback
                        self._codepoint = codepoint
        return (self._token.kind == TokenKind.INDENT or
                self._token.kind == TokenKind.DEDENT)




        """
        https://docs.python.org/3/reference/lexical_analysis.html#indentation
        The indentation levels of consecutive lines are used to generate INDENT and DEDENT tokens, using a stack, as follows.
        Before the first line of the file is read, a single zero is pushed on the stack; this will never be popped off again.
        The numbers pushed on the stack will always be strictly increasing from bottom to top. At the beginning of each logical line,
        the line’s indentation level is compared to the top of the stack. If it is equal, nothing happens. If it is larger,
        it is pushed on the stack, and one INDENT token is generated. If it is smaller, it must be one of the numbers occurring on the stack;
        all numbers on the stack that are larger are popped off, and for each number popped off a DEDENT token is generated.
        At the end of the file, a DEDENT token is generated for each number remaining on the stack that is larger than zero.
        """










from pptree import *
from io import StringIO
import sys
        temp_out = StringIO()
        sys.stdout = temp_out
        print_tree(tree, childattr='kids', nameattr='label', horizontal=True)
        sys.stdout = sys.__stdout__
        t = temp_out.getvalue()
        print(t)











        if self.accept(TokenKind.IDENTIFIER):
            root = ParseTreeFactory.create(ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER,
                                           ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER.name)
            root.symbol = SymbolFactory.create(root.label)
            root.symbol.grammar_symbol =\
                self._grammar.get_non_terminal(ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER.name)
            tree = root
            while True:
                if self._lexical_analyzer.token.kind == TokenKind.DOT:
                    la_token = self._lexical_analyzer.lookahead_lexeme()
                    if la_token.kind == TokenKind.IDENTIFIER:
                        kid = ParseTreeFactory.create(ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER,
                                                      ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER.name)
                        kid.symbol = SymbolFactory.create(kid.label)
                        kid.symbol.grammar_symbol =\
                            self._grammar.get_non_terminal(ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER.name)
                        tree.add_kid(kid)
                        kid = ParseTreeFactory.create(ParseTreeKind.IDENTIFIER,
                                                      ParseTreeKind.IDENTIFIER.name)
                        kid.symbol = SymbolFactory.create(kid.label)
                        kid.symbol.grammar_symbol =\
                            self._grammar.get_non_terminal(ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER.name)
                        kid.value = self._lexical_analyzer.token.literal
                        tree.add_kid(kid)
                        tree = tree.kids[0]
                        self._lexical_analyzer.next_lexeme()  # consume DOT
                        self._lexical_analyzer.next_lexeme()  # consume IDENTIFIER
                    else:
                        break
                elif self._lexical_analyzer.token.kind == TokenKind.IDENTIFIER:
                    kid = ParseTreeFactory.create(ParseTreeKind.IDENTIFIER,
                                                  ParseTreeKind.IDENTIFIER.name)
                    kid.symbol = SymbolFactory.create(kid.label)
                    kid.symbol.grammar_symbol = \
                        self._grammar.get_non_terminal(ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER.name)
                    kid.value = self._lexical_analyzer.token.literal
                    tree.add_kid(kid)
                    break
                else:
                    break
        else:
            root = ParseTreeFactory.create(ParseTreeKind.UNKNOWN,
                                           ParseTreeKind.UNKNOWN.name)
            root.symbol = SymbolFactory.create()
        return root






    def get_non_terminal(self, name):
        """
        """
        result = None
        normalized_name = Grammar.normalize_symbol_name(name)
        if normalized_name in self._pool:
            result = self._pool[normalized_name]
            if result.terminal:
                result = None
        return result




    def accept(self, visitor, *args, **kwargs):
        """
        """
        # visitor.visit(self, *args, **kwargs)
        if (self._flags & Flags.VISITED) != Flags.VISITED:
            self._flags = Flags.modify_flags(self._flags, Flags.VISITED, Flags.CLEAR)
            visitor.visit(self, *args, **kwargs)
            for kid in self._kids:
                if (kid.flags & Flags.VISITED) != Flags.VISITED:
                    kid.accept(visitor, *args, **kwargs)


    @staticmethod
    def to_string_fq_identifier(tree):
        """
        D:\Python\envs\python311-64bit\Lib\site-packages\pptree\pptree.py
        """
        class FqIdVisitor(ParseTreeVisitor):
            def __init__(self, _tree):
                """
                """
                super().__init__(_tree)
                self._data_sink = ''

            @property
            def data(self):
                """
                """
                return self._data_sink

            def visit(self, _tree, *args, **kwargs):
                """
                """
                if _tree.kind == ParseTreeKind.IDENTIFIER:
                    if self._data_sink:
                        self._data_sink += '.'
                    self._data_sink = f'{self._data_sink}{_tree.symbol.token.literal}'
                # if (_tree.flags & Flags.VISITED) != Flags.VISITED:
                #     _tree.flags = Flags.modify_flags(_tree.flags, Flags.VISITED, Flags.CLEAR)
                #     for kid in _tree.kids:
                #         if (kid.flags & Flags.VISITED) != Flags.VISITED:
                #             kid.accept(visitor, *args, **kwargs)
                #             # visitor.visit(kid, *args, **kwargs)
                return self._data_sink

        visitor = FqIdVisitor(tree)
        # result = visitor.visit(tree, '')
        tree.accept(visitor)
        return visitor.data




    def parse_fully_qualified_identifier(self):
        """
        fully_qualified_identifier : identifier
                                   | fully_qualified_identifier '.' identifier
                                   ;
        """
        if self.accept(TokenKind.IDENTIFIER):
            root = tree = ParseTreeFactory.make_tree(ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER,
                                                     self.grammar,
                                                     self._lexical_analyzer.prev_token)
            stack = deque()
            stack.append(self._lexical_analyzer.prev_token)  # push IDENTIFIER
            while not self._lexical_analyzer.eol():
                if self._lexical_analyzer.token.kind == TokenKind.DOT:
                    self._lexical_analyzer.next_lexeme()  # consume DOT
                elif self._lexical_analyzer.token.kind == TokenKind.IDENTIFIER:
                    stack.append(self._lexical_analyzer.token)  # push IDENTIFIER
                    self._lexical_analyzer.next_lexeme()  # consume IDENTIFIER
                else:
                    break
            while stack:
                token = stack.pop()
                if stack:
                    fq_kid = ParseTreeFactory.make_tree(ParseTreeKind.FULLY_QUALIFIED_IDENTIFIER,
                                                        self.grammar,
                                                        token)
                    tree.add_kid(fq_kid)
                    kid = ParseTreeFactory.make_tree(ParseTreeKind.IDENTIFIER,
                                                     self.grammar,
                                                     token)
                    tree.add_kid(kid)
                    tree = fq_kid
                else:
                    kid = ParseTreeFactory.make_tree(ParseTreeKind.IDENTIFIER,
                                                     self.grammar,
                                                     token)
                    tree.add_kid(kid)
        else:
            root = ParseTreeFactory.make_tree(ParseTreeKind.UNKNOWN,
                                              self.grammar,
                                              TokenFactory.UNKNOWN_TOKEN)
        return root





Profile
import cProfile, pstats, io
from pstats import SortKey

        pr = cProfile.Profile()
        pr.enable()
            GrammarAlgorithms.build_first_set(grammar, 2)
        pr.disable()
        s = io.StringIO()
        sortby = SortKey.CUMULATIVE
        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
        ps.print_stats()
        print(s.getvalue())


https://stackoverflow.com/questions/5061582/setting-stacksize-in-a-python-script
import resource, sys
resource.setrlimit(resource.RLIMIT_STACK, (2**29,-1))
sys.setrecursionlimit(10**6)


























    def accept(self, visitor, *args, **kwargs):
        """
        """
        def accept_action(_tree):
            visitor.visit(_tree, *args, **kwargs)

        preorder = kwargs.pop('preorder', False)
        postorder = kwargs.pop('postorder', False)
        preorder_action = None
        postorder_action = None
        if preorder:
            preorder_action = accept_action
        if postorder:
            postorder_action = accept_action
        GraphAlgorithms.calculate_tree_traverses(self,
                                                 preorder_action=preorder_action,
                                                 postorder_action=postorder_action)

    # def accept(self, visitor, *args, **kwargs):
    #     """
    #     """
    #     recursive = False
    #     if 'recursive' in kwargs:
    #         recursive = kwargs['recursive']
    #     if recursive:
    #         if (self._flags & Flags.VISITED) != Flags.VISITED:
    #             self._flags = Flags.modify_flags(self._flags, Flags.VISITED, Flags.CLEAR)
    #             visitor.visit(self, *args, **kwargs)
    #             for kid in self._kids:
    #                 if (kid.flags & Flags.VISITED) != Flags.VISITED:
    #                     kid.accept(visitor, *args, **kwargs)
    #     else:
    #         def default_action(_tree, *_args, **_kwargs):
    #             visitor.visit(_tree, *_args, **_kwargs)
    #
    #         preorder_action = kwargs.pop('preorder', '')
    #         postorder_action = None
    #         if not preorder_action:
    #             postorder_action = kwargs.pop('postorder', '')
    #             if not postorder_action:
    #                 preorder_action = default_action
    #         GraphAlgorithms.calculate_tree_traverses(self,
    #                                                  preorder_action=preorder_action,
    #                                                  postorder_action=postorder_action)









            def visit(self, cst, *args, **kwargs):  # noqa
                """
                """
                # papa = None
                stack = deque()
                # stack.append(TypeVisitor.TvpPair(cst, 1, papa))
                self.ast = papa = self.make_tree(cst)
                for kid in reversed(cst.kids):
                    if (TypeVisitor.accepted_non_terminal(kid.kind) or
                            TypeVisitor.accepted_terminal(kid.kind)):
                        stack.append(TypeVisitor.TvpPair(kid, 1, papa))
                while stack:
                    pair = stack.pop()
                    if pair.value == 1:
                        pair.value += 1
                        stack.append(pair)
                        papa = self.make_tree(pair.tree, pair.papa)
                        if papa:
                            # if self.ast.kind == ArtParseTreeKind.UNKNOWN:
                            #     self.ast = papa
                            for kid in reversed(pair.tree.kids):
                                if (TypeVisitor.accepted_non_terminal(kid.kind) or
                                        TypeVisitor.accepted_terminal(kid.kind)):
                                    stack.append(TypeVisitor.TvpPair(kid, 1, papa))
                    elif pair.value == 2:
                        pair.value += 1
                        stack.append(pair)










    def accept(self, visitor, *args, **kwargs):
        """
        """
        if (self._flags & Flags.VISITED) != Flags.VISITED:
            self._flags = Flags.modify_flags(self._flags, Flags.VISITED, Flags.CLEAR)
            visitor.visit(self, *args, **kwargs)
            for adjacence in self.adjacencies:
                if (adjacence.vertex.flags & Flags.VISITED) != Flags.VISITED:
                    adjacence.vertex.accept(visitor, *args, **kwargs)















        if tree:
            preorder, postorder = GraphAlgorithms.calculate_tree_traverses(tree)
            cached_nodes = dict()
            for node in preorder:
                cached_nodes[node.id] = Node(f'{get_label(node.symbol)}:{node.id}:{Text.generate_random_string(6)}:'
                                             f'attrs({DomainHelper.dict_to_string(node.attributes)})')
            nodes = list()
            for node in preorder:
                papa = None
                if node.papa:
                    papa = cached_nodes[node.papa.id]
                # new_node = Node(f'{get_label(node.symbol)}:'
                #                 f'attrs({DomainHelper.dict_to_string(node.attributes)})', parent=papa)
                new_node = cached_nodes[node.id]
                new_node.parent = papa
                kids = [cached_nodes[kid.id] for kid in node.kids]
                try:
                    new_node.children = kids
                except Exception as ex:
                    pass
                nodes.append(new_node)
            if nodes:
                DotExporter(nodes[0]).to_picture(filepath)







# from bigtree import Node, tree_to_dot
        if tree:
            preorder, postorder = GraphAlgorithms.calculate_tree_traverses(tree)
            cached_nodes = dict()
            for node in preorder:
                cached_nodes[node.id] = Node(f'{get_label(node.symbol)}:{node.id}:'
                                             f'attrs({DomainHelper.dict_to_string(node.attributes)})')
            nodes = list()
            for node in preorder:
                papa = None
                if node.papa:
                    papa = cached_nodes[node.papa.id]
                # new_node = Node(f'{get_label(node.symbol)}:'
                #                 f'attrs({DomainHelper.dict_to_string(node.attributes)})', parent=papa)
                new_node = cached_nodes[node.id]
                new_node.parent = papa
                kids = [cached_nodes[kid.id] for kid in node.kids]
                try:
                    new_node.children = kids
                except Exception as ex:
                    pass
                nodes.append(new_node)
            if nodes:
                # graph = tree_to_dot(nodes[0])
                # graph.write_png(filepath)
                DotExporter(nodes[0]).to_picture(filepath)









        # fq_identifier = tree = ArtAst.make_non_terminal_tree(ArtParseTreeKind.FULLY_QUALIFIED_IDENTIFIER,
        #                                                      self.grammar)
        # stack = deque()
        # stack.append(self.lexer.token)  # push IDENTIFIER
        # self.lexer.next_lexeme()
        # while not self.lexer.eos():
        #     if self.lexer.token.kind == TokenKind.DOT:
        #         la_token = self.lexer.lookahead_lexeme()
        #         if la_token.kind == TokenKind.IDENTIFIER:
        #             stack.append(self.lexer.token)  # push DOT
        #             stack.append(la_token)  # push IDENTIFIER
        #             self.lexer.next_lexeme()  # skip DOT
        #             self.lexer.next_lexeme()  # skip IDENTIFIER
        #         else:
        #             break
        #     else:
        #         break
        # while stack:
        #     token = stack.pop()
        #     if token.kind == TokenKind.DOT:
        #         self.consume_terminal(tree.papa, insert_index=1, token=token)
        #         continue
        #     if stack:
        #         fq_kid = ArtAst.make_non_terminal_tree(ArtParseTreeKind.FULLY_QUALIFIED_IDENTIFIER,
        #                                                self.grammar)
        #         tree.add_kid(fq_kid)
        #         self.consume_terminal(tree, ArtParseTreeKind.IDENTIFIER, token=token)
        #         tree = fq_kid
        #     else:
        #         self.consume_terminal(tree, ArtParseTreeKind.IDENTIFIER, token=token)





        la_token = self.lexer.lookahead_lexeme(skip=[TokenKind.WS,
                                                         TokenKind.EOL,
                                                         TokenKind.INDENT,
                                                         TokenKind.DEDENT,
                                                         TokenKind.CORRUPTED_DEDENT])








    def parse_type(self):
        """
        TYPE : integral_type array_type_rank_specifier_opt
             | type_name array_type_rank_specifier_opt
            #| type_parameter array_type_rank_specifier_opt
             ;
        """  # noqa
        self.inc_recursion_level()
        type_tree = ArtAst.make_non_terminal_tree(ArtParseTreeKind.TYPE, self.grammar)
        self.consume_noise(type_tree)
        if self.integral_type():
            self.consume_terminal(type_tree, ArtParseTreeKind.INTEGRAL_TYPE)
        else:
            if self.lexer.token.kind == TokenKind.IDENTIFIER:
                la_token = self.lexer.lookahead_lexeme(skip=[TokenKind.WS,
                                                             TokenKind.EOL,
                                                             TokenKind.INDENT,
                                                             TokenKind.DEDENT,
                                                             TokenKind.CORRUPTED_DEDENT])
                if la_token.kind == TokenKind.LESS_THAN_SIGN or la_token.kind == TokenKind.DOT:
                    type_name = self.parse_type_name()
                    type_tree.add_kid(type_name.tree)
                else:
                    type_parameter = self.parse_type_parameter()
                    type_tree.add_kid(type_parameter.tree)
            else:
                erroneous = self.syntax_error(type_tree,
                                              Status.INVALID_TOKEN,
                                              f'Expected IDENTIFIER and . or <, found '
                                              f'{self.lexer.token.kind.name}')
        array_type_rank_specifier_opt = self.parse_array_type_rank_specifier_opt()
        if array_type_rank_specifier_opt.status != ParseResult.Status.OPTIONAL:
            type_tree.add_kid(array_type_rank_specifier_opt.tree)
        self.dec_recursion_level()
        return ParseResult(ParseResult.Status.OK, type_tree)















    def parse_primary_expression(self):
        """
        primary_expression : literal
                           | identifier type_argument_seq_opt
                           | member_access
                           | array_element_access
                           | invocation_expression
                           | post_increment_expression
                           | post_decrement_expression
                           | object_creation_expression
                           | parenthesized_expression
                           ;
        """
        self.inc_recursion_level()
        primary_expression = ArtAst.make_non_terminal_tree(ArtParseTreeKind.PRIMARY_EXPRESSION, self.grammar)
        punctuator = False  #??
        invocation = False
        array_access = False
        state = ArtParseTreeKind.PRIMARY_EXPRESSION
        while True:
            self.consume_noise(primary_expression)
            if self.lexer.token.kind == TokenKind.IDENTIFIER:
                self.accept(primary_expression, TokenKind.IDENTIFIER, ArtParseTreeKind.IDENTIFIER)
                type_argument_seq_opt = self.parse_type_argument_seq_opt()
                if type_argument_seq_opt.status != ParseResult.Status.OPTIONAL:
                    primary_expression.add_kid(type_argument_seq_opt.tree)
                invocation = True
                array_access = True
                state = ArtParseTreeKind.MEMBER_ACCESS
            elif self.integral_type():
                self.consume_terminal(primary_expression, ArtParseTreeKind.INTEGRAL_TYPE)
                invocation = True
                array_access = True
                state = ArtParseTreeKind.MEMBER_ACCESS
            elif self.literal():
                self.parse_literal(primary_expression)
                invocation = True
                array_access = False
                state = ArtParseTreeKind.LITERAL
            elif self.lexer.token.kind == TokenKind.DOT:
                self.consume_terminal(primary_expression)
                invocation = False
                array_access = False
                state = ArtParseTreeKind.MEMBER_ACCESS
            elif self.lexer.token.kind == TokenKind.LEFT_PARENTHESIS:
                self.accept(primary_expression, TokenKind.LEFT_PARENTHESIS)
                if invocation:  # invocation_expression
                    arguments_opt = self.parse_arguments_opt()
                    if arguments_opt.status != ParseResult.Status.OPTIONAL:
                        primary_expression.add_kid(arguments_opt.tree)
                    state = ArtParseTreeKind.INVOCATION_EXPRESSION
                else:  # parenthesized_expression
                    expression = self.parse_expression()
                    primary_expression.add_kid(expression.tree)
                    state = ArtParseTreeKind.PARENTHESIZED_EXPRESSION
                self.consume_noise(primary_expression)
                self.accept(primary_expression, TokenKind.RIGHT_PARENTHESIS)
                invocation = False
                array_access = True
            elif self.lexer.token.kind == TokenKind.LEFT_SQUARE_BRACKET:
                self.consume_terminal(primary_expression)
                if array_access:  # array_element_access
                    arguments = self.parse_arguments()
                    primary_expression.add_kid(arguments.tree)
                    state = ArtParseTreeKind.ARRAY_ELEMENT_ACCESS
                else:  # array_creation_expression
                    array_type_specifier_opt = self.parse_array_type_specifier_opt()
                    if array_type_specifier_opt.status != ParseResult.Status.OPTIONAL:
                        primary_expression.add_kid(array_type_specifier_opt.tree)
                    # array_initializer
                    state = ArtParseTreeKind.ARRAY_CREATION_EXPRESSION
                self.consume_noise(primary_expression)
                self.accept(primary_expression, TokenKind.RIGHT_SQUARE_BRACKET)
                invocation = True
                array_access = True
            elif self.lexer.token.kind == TokenKind.LEFT_CURLY_BRACKET:
                self.consume_terminal(primary_expression)
                if saw_array_type_specifier:
                    array_initializer_opt = self.parse_array_initializer_opt()
                    if array_initializer_opt.status != ParseResult.Status.OPTIONAL:
                        primary_expression.add_kid(array_initializer_opt.tree)
                else:
                    arguments_opt = self.parse_arguments_opt()
                    if arguments_opt.status != ParseResult.Status.OPTIONAL:
                        primary_expression.add_kid(arguments_opt.tree)
                self.consume_noise(primary_expression)
                self.accept(primary_expression, TokenKind.RIGHT_CURLY_BRACKET)
                invocation = True
                array_access = False
                state = ArtParseTreeKind.OBJECT_CREATION_EXPRESSION
            elif self.lexer.token.kind == TokenKind.INCREMENT:
                self.consume_terminal(primary_expression)
                invocation = False
                array_access = False
                state = ArtParseTreeKind.POST_INCREMENT_EXPRESSION
            elif self.lexer.token.kind == TokenKind.DECREMENT:
                self.consume_terminal(primary_expression)
                invocation = False
                array_access = False
                state = ArtParseTreeKind.POST_DECREMENT_EXPRESSION
            else:
                break
        self.current_expression = ArtParseTreeKind.PRIMARY_EXPRESSION
        self.dec_recursion_level()
        return ParseResult(ParseResult.Status.OK, primary_expression)




    def parse_member_access(self):
        """
        member_access : integral_type '.' identifier type_argument_seq_opt
                      | primary_expression '.' identifier type_argument_seq_opt
                      ;
        """
        self.inc_recursion_level()
        member_access = ArtAst.make_non_terminal_tree(ArtParseTreeKind.MEMBER_ACCESS, self.grammar)
        self.consume_noise(member_access)
        if self.integral_type():
            self.consume_terminal(member_access, ArtParseTreeKind.INTEGRAL_TYPE)
        else:
            primary_expression = self.parse_primary_expression()
            member_access.add_kid(primary_expression)
        self.consume_noise(member_access)
        if self.lexer.token.kind == TokenKind.DOT:
            self.consume_terminal(member_access)
            self.consume_noise(member_access)
            self.accept(primary_expression, TokenKind.IDENTIFIER, ArtParseTreeKind.IDENTIFIER)
            type_argument_seq_opt = self.parse_type_argument_seq_opt()
            if type_argument_seq_opt.status != ParseResult.Status.OPTIONAL:
                primary_expression.add_kid(type_argument_seq_opt.tree)
        self.dec_recursion_level()
        return ParseResult(ParseResult.Status.OK, member_access)





    def load_file(self, filepath):
        """
        """
        with open(os.path.abspath(filepath), 'r') as stream:
            content = stream.read()
            return self.load(content)









            la = self.grammar.lookup_symbol(grammar_symbol_name).la
            # s = ','.join([f'[TokenKind.{la_set[0].token.name}]' for la_set in la if la_set])
            return ArtParser.MatchLaResult(la_token.kind in [la_set[0].token for la_set in la if la_set],
                                           la_token,
                                           la_token.kind == TokenKind.EOS)







    def rewind(self):
        """
        Restore the last saved state for backtracking.
        Usually called by lexical analyzers.
        """
        if self.snapshots:
            self.token.reset()
            self.content_position = self.snapshots.pop()
            self.lexeme_position = self.content_position
            self.codepoint = Text.eos_codepoint()
            self.advance()










        self.snapshots = deque()  # stack of backtracking snapshots - positions

    def snapshot(self, offset=0, persist=True):
        """
        Snapshot the current state for backtracking.
        Usually called by parsers.
        """
        state = self.tokenizer.snapshot(offset, persist=persist)
        state = (state,
                 deepcopy(self.token),
                 deepcopy(self.prev_token))
        if persist:
            self.snapshots.append(state)
        return state

    def rewind(self):
        """
        Restore the last saved state for backtracking.
        Usually called by parsers.
        """
        if self.snapshots:
            state = self.snapshots.pop()
            self.tokenizer.rewind()
            self.token = deepcopy(state[1])
            self.prev_token = deepcopy(state[2])

    def discard(self):
        """
        Discard the last saved state for backtracking.
        Usually called by parsers.
        """
        if self.snapshots:
            self.tokenizer.discard()
            self.snapshots.pop()





            mask16 = (1 << 16) - 1




https://stackoverflow.com/questions/21843693/creating-stream-to-iterate-over-from-string-in-python
from io import StringIO
s = StringIO("a\t\b\nc\td\n")
for line in s:
    print(line)




    def struct_pack_template(self):
        """
        """
        count = len(bytes(self.version, 'utf-8'))
        return f'{count}s'



D:\Tmp\Python.Samples\bigtree\node\basenode.py
    def __check_parent_loop(self, new_parent: T) -> None:
        """Check parent type

        Args:
            new_parent (Self): parent node
        """
        if new_parent is not None:
            if new_parent is self:
                raise LoopError("Error setting parent: Node cannot be parent of itself")
            if any(
                ancestor is self
                for ancestor in new_parent.ancestors
                if new_parent.ancestors
            ):
                raise LoopError(
                    "Error setting parent: Node cannot be ancestor of itself"
                )





assert type(command) is command, f"Invalid argument type {command}, Command is expected."



        handlers = {k: v for k, v in self.notification_handlers.items() if k == type(notification)}
        return self.notification_publisher.publish(handlers, notification)



